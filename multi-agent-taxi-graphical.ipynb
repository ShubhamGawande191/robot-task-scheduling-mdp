{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.10.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pygame\n",
    "from io import StringIO\n",
    "from contextlib import closing\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces, Env\n",
    "from gymnasium.envs.toy_text.utils import categorical_sample\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/\n"
     ]
    }
   ],
   "source": [
    "# Set up TensorBoard logging\n",
    "log_dir = \"./logs/\"\n",
    "new_logger = configure(log_dir, [\"stdout\", \"tensorboard\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP = [\n",
    "    \"+---------+\",\n",
    "    \"|R: | : :G|\",\n",
    "    \"| :C| : :M|\",\n",
    "    \"| : : : : |\",\n",
    "    \"| | : | : |\",\n",
    "    \"|Y| : |B: |\",\n",
    "    \"+---------+\",\n",
    "]\n",
    "WINDOW_SIZE = (550, 350)\n",
    "FRAME_RATE = 10  # Frames per second\n",
    "DELAY = 0.1  # Delay in seconds\n",
    "\n",
    "class MultiAgentTaxiEnv(Env):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"ansi\", \"rgb_array\"],\n",
    "        \"render_fps\": FRAME_RATE,\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode=None, num_agents=2, num_tasks=3):\n",
    "        super(MultiAgentTaxiEnv, self).__init__()\n",
    "        self.desc = np.asarray(MAP, dtype=\"c\")\n",
    "        self.num_agents = num_agents\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.locs = locs = [(0, 0), (0, 4), (4, 0), (4, 3), (1, 2), (1, 5)]\n",
    "        self.locs_colors = [(255, 0, 0), (0, 255, 0), (255, 255, 0), (0, 0, 255), (0, 255, 255), (255, 0, 255)]\n",
    "\n",
    "        self.action_space = spaces.MultiDiscrete([6] * self.num_agents)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=5, shape=(self.num_agents * 3 + self.num_tasks * 4,), dtype=np.int32\n",
    "        )\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # pygame utils\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.cell_size = (\n",
    "            WINDOW_SIZE[0] / self.desc.shape[1],\n",
    "            WINDOW_SIZE[1] / self.desc.shape[0],\n",
    "        )\n",
    "        self.taxi_imgs = None\n",
    "        self.passenger_img = None\n",
    "        self.destination_img = None\n",
    "        self.median_horiz = None\n",
    "        self.median_vert = None\n",
    "        self.background_img = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.agents = np.zeros((self.num_agents, 3), dtype=np.int32)\n",
    "        self.agents[:, :2] = np.random.randint(0, 5, (self.num_agents, 2))\n",
    "\n",
    "        self.tasks = np.zeros((self.num_tasks, 4), dtype=np.int32)\n",
    "        for i in range(self.num_tasks):\n",
    "            pickup_loc = self.locs[np.random.randint(0, len(self.locs))]\n",
    "            dropoff_loc = self.locs[np.random.randint(0, len(self.locs))]\n",
    "            while dropoff_loc == pickup_loc:\n",
    "                dropoff_loc = self.locs[np.random.randint(0, len(self.locs))]\n",
    "            self.tasks[i] = np.array([pickup_loc[0], pickup_loc[1], dropoff_loc[0], dropoff_loc[1]])\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.concatenate([self.agents.flatten(), self.tasks.flatten()])\n",
    "\n",
    "    def step(self, actions):\n",
    "        assert self.action_space.contains(actions), f\"Invalid action: {actions}\"\n",
    "\n",
    "        rewards = np.zeros(self.num_agents)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        for i in range(self.num_agents):\n",
    "            if actions[i] == 0:  # Move up\n",
    "                self.agents[i, 1] = max(self.agents[i, 1] - 1, 0)\n",
    "            elif actions[i] == 1:  # Move down\n",
    "                self.agents[i, 1] = min(self.agents[i, 1] + 1, 4)\n",
    "            elif actions[i] == 2:  # Move left\n",
    "                self.agents[i, 0] = max(self.agents[i, 0] - 1, 0)\n",
    "            elif actions[i] == 3:  # Move right\n",
    "                self.agents[i, 0] = min(self.agents[i, 0] + 1, 4)\n",
    "            elif actions[i] == 4:  # Pick up task\n",
    "                for task_id, task in enumerate(self.tasks):\n",
    "                    if np.array_equal(self.agents[i, :2], task[:2]) and self.agents[i, 2] == 0:\n",
    "                        self.agents[i, 2] = task_id + 1  # Carry task_id + 1\n",
    "                        rewards[i] += 10  # Reward for successful pickup\n",
    "                        break  # Stop checking other tasks once one is picked up\n",
    "            elif actions[i] == 5:  # Drop off task\n",
    "                task_id = self.agents[i, 2] - 1\n",
    "                if task_id >= 0 and np.array_equal(self.agents[i, :2], self.tasks[task_id, 2:]):\n",
    "                    rewards[i] += 20  # Reward for successful drop-off\n",
    "                    self.tasks[task_id] = np.array([-1, -1, -1, -1])  # Invalidate the task\n",
    "                    self.agents[i, 2] = 0  # No longer carrying task\n",
    "                else:\n",
    "                    rewards[i] -= 10  # Penalty for illegal drop-off\n",
    "\n",
    "            # Penalize for each step to encourage efficiency\n",
    "            rewards[i] -= 1\n",
    "\n",
    "        # Check if all tasks are completed\n",
    "        if all(self.tasks[:, 0] == -1):\n",
    "            terminated = True\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        return self._get_obs(), np.sum(rewards), terminated, truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        try:\n",
    "            import pygame  # dependency to pygame only if rendering with human\n",
    "        except ImportError as e:\n",
    "            raise DependencyNotInstalled(\n",
    "                'pygame is not installed, run `pip install \"gymnasium[toy-text]\"`'\n",
    "            ) from e\n",
    "\n",
    "        if self.window is None:\n",
    "            pygame.init()\n",
    "            pygame.display.set_caption(\"Multi-Agent Taxi\")\n",
    "            if mode == \"human\":\n",
    "                self.window = pygame.display.set_mode(WINDOW_SIZE)\n",
    "            elif mode == \"rgb_array\":\n",
    "                self.window = pygame.Surface(WINDOW_SIZE)\n",
    "\n",
    "        assert (\n",
    "            self.window is not None\n",
    "        ), \"Something went wrong with pygame. This should never happen.\"\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "        if self.taxi_imgs is None:\n",
    "            file_names = [\n",
    "                \"img/cab_front.png\",\n",
    "                \"img/cab_rear.png\",\n",
    "                \"img/cab_right.png\",\n",
    "                \"img/cab_left.png\",\n",
    "            ]\n",
    "            self.taxi_imgs = [\n",
    "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
    "                for file_name in file_names\n",
    "            ]\n",
    "        if self.passenger_img is None:\n",
    "            file_name = \"img/passenger.png\"\n",
    "            self.passenger_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "        if self.destination_img is None:\n",
    "            file_name = \"img/hotel.png\"\n",
    "            self.destination_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "            self.destination_img.set_alpha(170)\n",
    "        if self.median_horiz is None:\n",
    "            file_names = [\n",
    "                \"img/gridworld_median_left.png\",\n",
    "                \"img/gridworld_median_horiz.png\",\n",
    "                \"img/gridworld_median_right.png\",\n",
    "            ]\n",
    "            self.median_horiz = [\n",
    "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
    "                for file_name in file_names\n",
    "            ]\n",
    "        if self.median_vert is None:\n",
    "            file_names = [\n",
    "                \"img/gridworld_median_top.png\",\n",
    "                \"img/gridworld_median_vert.png\",\n",
    "                \"img/gridworld_median_bottom.png\",\n",
    "            ]\n",
    "            self.median_vert = [\n",
    "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
    "                for file_name in file_names\n",
    "            ]\n",
    "        if self.background_img is None:\n",
    "            file_name = \"img/taxi_background.png\"\n",
    "            self.background_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "\n",
    "        desc = self.desc\n",
    "\n",
    "        for y in range(0, desc.shape[0]):\n",
    "            for x in range(0, desc.shape[1]):\n",
    "                cell = (x * self.cell_size[0], y * self.cell_size[1])\n",
    "                self.window.blit(self.background_img, cell)\n",
    "                if desc[y][x] == b\"|\" and (y == 0 or desc[y - 1][x] != b\"|\"):\n",
    "                    self.window.blit(self.median_vert[0], cell)\n",
    "                elif desc[y][x] == b\"|\" and (\n",
    "                    y == desc.shape[0] - 1 or desc[y + 1][x] != b\"|\"\n",
    "                ):\n",
    "                    self.window.blit(self.median_vert[2], cell)\n",
    "                elif desc[y][x] == b\"|\":\n",
    "                    self.window.blit(self.median_vert[1], cell)\n",
    "                elif desc[y][x] == b\"-\" and (x == 0 or desc[y][x - 1] != b\"-\"):\n",
    "                    self.window.blit(self.median_horiz[0], cell)\n",
    "                elif desc[y][x] == b\"-\" and (\n",
    "                    x == desc.shape[1] - 1 or desc[y][x + 1] != b\"-\"\n",
    "                ):\n",
    "                    self.window.blit(self.median_horiz[2], cell)\n",
    "                elif desc[y][x] == b\"-\":\n",
    "                    self.window.blit(self.median_horiz[1], cell)\n",
    "\n",
    "        for cell, color in zip(self.locs, self.locs_colors):\n",
    "            color_cell = pygame.Surface(self.cell_size)\n",
    "            color_cell.set_alpha(128)\n",
    "            color_cell.fill(color)\n",
    "            loc = self.get_surf_loc(cell)\n",
    "            self.window.blit(color_cell, (loc[0], loc[1] + 10))\n",
    "\n",
    "        for agent in self.agents:\n",
    "            self.window.blit(self.taxi_imgs[0], self.get_surf_loc((agent[0], agent[1])))\n",
    "\n",
    "        for task in self.tasks:\n",
    "            if task[0] != -1:\n",
    "                self.window.blit(self.passenger_img, self.get_surf_loc((task[0], task[1])))\n",
    "                self.window.blit(self.destination_img, self.get_surf_loc((task[2], task[3])))\n",
    "\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def get_surf_loc(self, map_loc):\n",
    "        return (map_loc[1] * 2 + 1) * self.cell_size[0], (\n",
    "            map_loc[0] + 1\n",
    "        ) * self.cell_size[1]\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            import pygame\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the environment\n",
    "# env = MultiAgentTaxiEnv(render_mode=\"human\", num_agents=2, num_tasks=3)\n",
    "# obs = env.reset()\n",
    "# done = False\n",
    "\n",
    "# DELAY = 0.1\n",
    "\n",
    "# while not done:\n",
    "#     action = env.action_space.sample()\n",
    "#     obs, reward, terminated, truncated, info = env.step(action)\n",
    "#     done = terminated or truncated\n",
    "#     env.render()\n",
    "#     time.sleep(DELAY)\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the environment\n",
    "base_env = MultiAgentTaxiEnv(render_mode=\"human\", num_agents=2, num_tasks=3)\n",
    "\n",
    "# Check if the environment follows the Gymnasium API\n",
    "check_env(base_env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = DummyVecEnv([lambda: Monitor(MultiAgentTaxiEnv(render_mode=\"human\", num_agents=2, num_tasks=3))])\n",
    "\n",
    "# # create the model\n",
    "# model = PPO('MlpPolicy', env, verbose=1)\n",
    "# model.set_logger(new_logger)\n",
    "\n",
    "# # Setup evaluation callback\n",
    "# eval_env = DummyVecEnv([lambda: Monitor(MultiAgentTaxiEnv(render_mode=\"human\", num_agents=2, num_tasks=3))])\n",
    "# eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
    "#                              log_path='./logs/', eval_freq=500,\n",
    "#                              deterministic=True, render=False)\n",
    "\n",
    "# # train the model\n",
    "# model.learn(total_timesteps=10000, callback=eval_callback)\n",
    "\n",
    "# # save the model\n",
    "# model.save(\"ppo_multi_agent_taxi\")\n",
    "\n",
    "# # load the model\n",
    "# model = PPO.load(\"ppo_multi_agent_taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and check the base environment\n",
    "base_env = MultiAgentTaxiEnv(render_mode=\"human\", num_agents=2, num_tasks=3)\n",
    "check_env(base_env, warn=True)\n",
    "\n",
    "# Initialize the custom environment and wrap it with Monitor and DummyVecEnv\n",
    "env = DummyVecEnv([lambda: Monitor(MultiAgentTaxiEnv(render_mode=\"human\", num_agents=2, num_tasks=3))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beelzebub/.local/lib/python3.10/site-packages/stable_baselines3/common/policies.py:460: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 169  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 12   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025538277 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.56       |\n",
      "|    explained_variance   | -0.00328    |\n",
      "|    learning_rate        | 0.003       |\n",
      "|    loss                 | 204         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0405     |\n",
      "|    value_loss           | 2.6e+03     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 38         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.21316075 |\n",
      "|    clip_fraction        | 0.451      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.48      |\n",
      "|    explained_variance   | 4.47e-06   |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | 60.7       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0122    |\n",
      "|    value_loss           | 801        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061192818 |\n",
      "|    clip_fraction        | 0.564       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.19       |\n",
      "|    explained_variance   | -2.74e-06   |\n",
      "|    learning_rate        | 0.003       |\n",
      "|    loss                 | 60.1        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.0352      |\n",
      "|    value_loss           | 117         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 63         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05095078 |\n",
      "|    clip_fraction        | 0.451      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.03      |\n",
      "|    explained_variance   | -3.1e-06   |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | 18.1       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | 0.0101     |\n",
      "|    value_loss           | 39.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 76          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049284615 |\n",
      "|    clip_fraction        | 0.46        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.98       |\n",
      "|    explained_variance   | -2.5e-06    |\n",
      "|    learning_rate        | 0.003       |\n",
      "|    loss                 | 6.8         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 0.0154      |\n",
      "|    value_loss           | 14.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mset_logger(new_logger)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_multi_agent_taxi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:169\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 169\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/policies.py:619\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    617\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(obs)\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[0;32m--> 619\u001b[0m     latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m     pi_features, vf_features \u001b[38;5;241m=\u001b[39m features\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/torch_layers.py:222\u001b[0m, in \u001b[0;36mMlpExtractor.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[th\u001b[38;5;241m.\u001b[39mTensor, th\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m    :return: latent_policy, latent_value of the specified network.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m        If all layers are shared, then ``latent_policy == latent_value``\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_critic(features)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/torch_layers.py:225\u001b[0m, in \u001b[0;36mMlpExtractor.forward_actor\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_actor\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py:356\u001b[0m, in \u001b[0;36mTanh.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set up TensorBoard logging\n",
    "log_dir = \"./logs/\"\n",
    "new_logger = configure(log_dir, [\"stdout\", \"tensorboard\"])\n",
    "\n",
    "# PPO model parameters\n",
    "total_timesteps = 1338900\n",
    "\n",
    "# Create the PPO model and set the logger\n",
    "model = PPO('MlpPolicy', env, verbose=1,\n",
    "            batch_size=64,\n",
    "            ent_coef=0.1,\n",
    "            gamma=0.985,\n",
    "            learning_rate=0.003,\n",
    "            n_steps=2048,\n",
    "            n_epochs=10,\n",
    "            policy_kwargs=dict(net_arch=[dict(pi=[64, 64], vf=[64, 64])]))\n",
    "\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_multi_agent_taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     11\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDELAY\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Control the speed of the simulation\u001b[39;00m\n\u001b[1;32m     14\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = PPO.load(\"ppo_multi_agent_taxi\")\n",
    "\n",
    "# test the model\n",
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    time.sleep(DELAY)  # Control the speed of the simulation\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP = [\n",
    "    \"+---------+\",\n",
    "    \"|R: | : :G|\",\n",
    "    \"| :C| : :M|\",\n",
    "    \"| : : : : |\",\n",
    "    \"| | : | : |\",\n",
    "    \"|Y| : |B: |\",\n",
    "    \"+---------+\",\n",
    "]\n",
    "WINDOW_SIZE = (550, 350)\n",
    "FRAME_RATE = 10  # Frames per second\n",
    "DELAY = 0.1  # Delay in seconds\n",
    "\n",
    "class MultiAgentTaxiEnv(Env):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"ansi\", \"rgb_array\"],\n",
    "        \"render_fps\": FRAME_RATE,\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode=None, num_agents=2, num_tasks=3):\n",
    "        super(MultiAgentTaxiEnv, self).__init__()\n",
    "        self.desc = np.asarray(MAP, dtype=\"c\")\n",
    "        self.num_agents = num_agents\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.locs = locs = [(0, 0), (0, 4), (4, 0), (4, 3), (1, 2), (1, 5)]\n",
    "        self.locs_colors = [(255, 0, 0), (0, 255, 0), (255, 255, 0), (0, 0, 255), (0, 255, 255), (255, 0, 255)]\n",
    "\n",
    "        self.action_space = spaces.MultiDiscrete([6] * self.num_agents)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=5, shape=(self.num_agents * 3 + self.num_tasks * 4,), dtype=np.int32\n",
    "        )\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # pygame utils\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.cell_size = (\n",
    "            WINDOW_SIZE[0] / self.desc.shape[1],\n",
    "            WINDOW_SIZE[1] / self.desc.shape[0],\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.agents = np.zeros((self.num_agents, 3), dtype=np.int32)\n",
    "        self.agents[:, :2] = np.random.randint(0, 5, (self.num_agents, 2))\n",
    "\n",
    "        self.tasks = np.zeros((self.num_tasks, 4), dtype=np.int32)\n",
    "        for i in range(self.num_tasks):\n",
    "            pickup_loc = self.locs[np.random.randint(0, len(self.locs))]\n",
    "            dropoff_loc = self.locs[np.random.randint(0, len(self.locs))]\n",
    "            while dropoff_loc == pickup_loc:\n",
    "                dropoff_loc = self.locs[np.random.randint(0, len(self.locs))]\n",
    "            self.tasks[i] = np.array([pickup_loc[0], pickup_loc[1], dropoff_loc[0], dropoff_loc[1]])\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.concatenate([self.agents.flatten(), self.tasks.flatten()])\n",
    "\n",
    "    def step(self, actions):\n",
    "        assert self.action_space.contains(actions), f\"Invalid action: {actions}\"\n",
    "\n",
    "        rewards = np.zeros(self.num_agents)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        for i in range(self.num_agents):\n",
    "            if actions[i] == 0:  # Move up\n",
    "                self.agents[i, 1] = max(self.agents[i, 1] - 1, 0)\n",
    "            elif actions[i] == 1:  # Move down\n",
    "                self.agents[i, 1] = min(self.agents[i, 1] + 1, 4)\n",
    "            elif actions[i] == 2:  # Move left\n",
    "                self.agents[i, 0] = max(self.agents[i, 0] - 1, 0)\n",
    "            elif actions[i] == 3:  # Move right\n",
    "                self.agents[i, 0] = min(self.agents[i, 0] + 1, 4)\n",
    "            elif actions[i] == 4:  # Pick up task\n",
    "                for task_id, task in enumerate(self.tasks):\n",
    "                    if np.array_equal(self.agents[i, :2], task[:2]) and self.agents[i, 2] == 0:\n",
    "                        self.agents[i, 2] = task_id + 1  # Carry task_id + 1\n",
    "                        rewards[i] += 10  # Reward for successful pickup\n",
    "                        break  # Stop checking other tasks once one is picked up\n",
    "            elif actions[i] == 5:  # Drop off task\n",
    "                task_id = self.agents[i, 2] - 1\n",
    "                if task_id >= 0 and np.array_equal(self.agents[i, :2], self.tasks[task_id, 2:]):\n",
    "                    rewards[i] += 20  # Reward for successful drop-off\n",
    "                    self.tasks[task_id] = np.array([-1, -1, -1, -1])  # Invalidate the task\n",
    "                    self.agents[i, 2] = 0  # No longer carrying task\n",
    "                else:\n",
    "                    rewards[i] -= 10  # Penalty for illegal drop-off\n",
    "\n",
    "            # Penalize for each step to encourage efficiency\n",
    "            rewards[i] -= 1\n",
    "\n",
    "        # Check if all tasks are completed\n",
    "        if all(self.tasks[:, 0] == -1):\n",
    "            terminated = True\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        return self._get_obs(), np.sum(rewards), terminated, truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        try:\n",
    "            import pygame  # dependency to pygame only if rendering with human\n",
    "        except ImportError as e:\n",
    "            raise DependencyNotInstalled(\n",
    "                'pygame is not installed, run `pip install \"gymnasium[toy-text]\"`'\n",
    "            ) from e\n",
    "\n",
    "        if self.window is None:\n",
    "            pygame.init()\n",
    "            pygame.display.set_caption(\"Multi-Agent Taxi\")\n",
    "            if mode == \"human\":\n",
    "                self.window = pygame.display.set_mode(WINDOW_SIZE)\n",
    "            elif mode == \"rgb_array\":\n",
    "                self.window = pygame.Surface(WINDOW_SIZE)\n",
    "\n",
    "        assert (\n",
    "            self.window is not None\n",
    "        ), \"Something went wrong with pygame. This should never happen.\"\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "        if self.taxi_imgs is None:\n",
    "            file_names = [\n",
    "                \"img/cab_front.png\",\n",
    "                \"img/cab_rear.png\",\n",
    "                \"img/cab_right.png\",\n",
    "                \"img/cab_left.png\",\n",
    "            ]\n",
    "            self.taxi_imgs = [\n",
    "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
    "                for file_name in file_names\n",
    "            ]\n",
    "        if self.passenger_img is None:\n",
    "            file_name = \"img/passenger.png\"\n",
    "            self.passenger_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "        if self.destination_img is None:\n",
    "            file_name = \"img/hotel.png\"\n",
    "            self.destination_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "            self.destination_img.set_alpha(170)\n",
    "        if self.median_horiz is None:\n",
    "            file_names = [\n",
    "                \"img/gridworld_median_left.png\",\n",
    "                \"img/gridworld_median_horiz.png\",\n",
    "                \"img/gridworld_median_right.png\",\n",
    "            ]\n",
    "            self.median_horiz = [\n",
    "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
    "                for file_name in file_names\n",
    "            ]\n",
    "        if self.median_vert is None:\n",
    "            file_names = [\n",
    "                \"img/gridworld_median_top.png\",\n",
    "                \"img/gridworld_median_vert.png\",\n",
    "                \"img/gridworld_median_bottom.png\",\n",
    "            ]\n",
    "            self.median_vert = [\n",
    "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
    "                for file_name in file_names\n",
    "            ]\n",
    "        if self.background_img is None:\n",
    "            file_name = \"img/taxi_background.png\"\n",
    "            self.background_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "\n",
    "        desc = self.desc\n",
    "\n",
    "        for y in range(0, desc.shape[0]):\n",
    "            for x in range(0, desc.shape[1]):\n",
    "                cell = (x * self.cell_size[0], y * self.cell_size[1])\n",
    "                self.window.blit(self.background_img, cell)\n",
    "                if desc[y][x] == b\"|\" and (y == 0 or desc[y - 1][x] != b\"|\"):\n",
    "                    self.window.blit(self.median_vert[0], cell)\n",
    "                elif desc[y][x] == b\"|\" and (\n",
    "                    y == desc.shape[0] - 1 or desc[y + 1][x] != b\"|\"\n",
    "                ):\n",
    "                    self.window.blit(self.median_vert[2], cell)\n",
    "                elif desc[y][x] == b\"|\":\n",
    "                    self.window.blit(self.median_vert[1], cell)\n",
    "                elif desc[y][x] == b\"-\" and (x == 0 or desc[y][x - 1] != b\"-\"):\n",
    "                    self.window.blit(self.median_horiz[0], cell)\n",
    "                elif desc[y][x] == b\"-\" and (\n",
    "                    x == desc.shape[1] - 1 or desc[y][x + 1] != b\"-\"\n",
    "                ):\n",
    "                    self.window.blit(self.median_horiz[2], cell)\n",
    "                elif desc[y][x] == b\"-\":\n",
    "                    self.window.blit(self.median_horiz[1], cell)\n",
    "\n",
    "        for cell, color in zip(self.locs, self.locs_colors):\n",
    "            color_cell = pygame.Surface(self.cell_size)\n",
    "            color_cell.set_alpha(128)\n",
    "            color_cell.fill(color)\n",
    "            loc = self.get_surf_loc(cell)\n",
    "            self.window.blit(color_cell, (loc[0], loc[1] + 10))\n",
    "\n",
    "        for agent in self.agents:\n",
    "            self.window.blit(self.taxi_imgs[0], self.get_surf_loc((agent[0], agent[1])))\n",
    "\n",
    "        for task in self.tasks:\n",
    "            if task[0] != -1:\n",
    "                self.window.blit(self.passenger_img, self.get_surf_loc((task[0], task[1])))\n",
    "                self.window.blit(self.destination_img, self.get_surf_loc((task[2], task[3])))\n",
    "\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def get_surf_loc(self, map_loc):\n",
    "        return (map_loc[1] * 2 + 1) * self.cell_size[0], (\n",
    "            map_loc[0] + 1\n",
    "        ) * self.cell_size[1]\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            import pygame\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "\n",
    "    def _compute_transition_and_rewards(self):\n",
    "        num_states = self.observation_space.shape[0]\n",
    "        num_actions_per_agent = self.action_space.nvec[0]\n",
    "        num_actions = num_actions_per_agent ** self.num_agents\n",
    "\n",
    "        P = np.zeros((num_actions, num_states, num_states))\n",
    "        R = np.zeros((num_states, num_actions))\n",
    "\n",
    "        # Iterate over all states and actions\n",
    "        for state in range(num_states):\n",
    "            for action in range(num_actions):\n",
    "                # Decompose the multi-agent action into individual agent actions\n",
    "                individual_actions = np.unravel_index(action, [num_actions_per_agent] * self.num_agents)\n",
    "                \n",
    "                # Simulate the environment with the given state and action\n",
    "                self.reset()\n",
    "                self.state = state\n",
    "                \n",
    "                # Handle different return values of step method\n",
    "                result = self.step(individual_actions)\n",
    "                if len(result) == 3:\n",
    "                    next_state, reward, done = result\n",
    "                elif len(result) == 4:\n",
    "                    next_state, reward, done, _ = result\n",
    "                else:\n",
    "                    next_state, reward, done, truncated, _ = result\n",
    "                \n",
    "                P[action, state, next_state] += 1\n",
    "                R[state, action] = reward\n",
    "\n",
    "        # Normalize the transition matrix\n",
    "        for action in range(num_actions):\n",
    "            P[action] /= P[action].sum(axis=1, keepdims=True)\n",
    "        \n",
    "        return P, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy: (4, 24, 10, 0, 6, 9, 3, 28, 28, 28, 4, 7, 1, 28, 1, 10, 1, 4)\n",
      "Value Function: (47.284824163058445, 47.0281972753195, 47.028197275319506, 37.0281972753195, 38.45390220720249, 38.45390220720249, 38.45390220720249, 38.45390220720249, 47.284824163058445, 46.29158306051329, 47.028197275319506, 37.284824163058445, 37.028197275319506, 46.29158306051329, 37.284824163058445, 47.284824163058445, 38.45390220720249, 46.29158306051329)\n"
     ]
    }
   ],
   "source": [
    "import mdptoolbox\n",
    "\n",
    "# Initialize the environment\n",
    "env = MultiAgentTaxiEnv()\n",
    "\n",
    "# Compute transition and reward matrices\n",
    "P, R = env._compute_transition_and_rewards()\n",
    "\n",
    "# Define discount factor\n",
    "discount_factor = 0.9\n",
    "\n",
    "# Initialize Policy Iteration\n",
    "pi = mdptoolbox.mdp.PolicyIteration(P, R, discount_factor)\n",
    "\n",
    "# Run Policy Iteration\n",
    "pi.run()\n",
    "\n",
    "# Print the optimal policy and value function\n",
    "print(f\"Optimal Policy: {pi.policy}\")\n",
    "print(f\"Value Function: {pi.V}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "Reached maximum steps without finishing episode.\n",
      "Episode 1: Total Reward: -1990.0\n",
      "Episode: 2\n",
      "Reached maximum steps without finishing episode.\n",
      "Episode 2: Total Reward: -2000.0\n",
      "Episode: 3\n",
      "Reached maximum steps without finishing episode.\n",
      "Episode 3: Total Reward: -2000.0\n",
      "Episode: 4\n",
      "Reached maximum steps without finishing episode.\n",
      "Episode 4: Total Reward: -2000.0\n",
      "Episode: 5\n",
      "Reached maximum steps without finishing episode.\n",
      "Episode 5: Total Reward: -2000.0\n",
      "Episode: 6\n",
      "Reached maximum steps without finishing episode.\n",
      "Episode 6: Total Reward: -2000.0\n",
      "Episode: 7\n",
      "Reached maximum steps without finishing episode.\n",
      "Episode 7: Total Reward: -2000.0\n",
      "Episode: 8\n",
      "Reached maximum steps without finishing episode.\n",
      "Episode 8: Total Reward: -2000.0\n",
      "Episode: 9\n",
      "Reached maximum steps without finishing episode.\n",
      "Episode 9: Total Reward: -2000.0\n",
      "Episode: 10\n",
      "Reached maximum steps without finishing episode.\n",
      "Episode 10: Total Reward: -2000.0\n"
     ]
    }
   ],
   "source": [
    "def test_policy(env, policy, num_episodes=10, max_steps=1000):\n",
    "    for episode in range(num_episodes):\n",
    "        print(\"Episode:\", episode + 1)\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step = 0  # Initialize step counter\n",
    "        while not done and step < max_steps:\n",
    "            if not isinstance(obs, (tuple, list)):\n",
    "                obs = (obs,)\n",
    "            if len(obs) != len(env.observation_space.shape):\n",
    "                raise ValueError(\"obs dimensions do not match env.observation_space.shape\")\n",
    "            try:\n",
    "                state_index = np.ravel_multi_index(obs, env.observation_space.shape)[0]\n",
    "                action_index = policy[int(state_index)]\n",
    "                actions = np.unravel_index(action_index, env.action_space.nvec)\n",
    "                obs, reward, done, _, _ = env.step(actions)\n",
    "                total_reward += reward\n",
    "                step += 1  # Increment step counter\n",
    "            except ValueError as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                break\n",
    "        if step >= max_steps:\n",
    "            print(\"Reached maximum steps without finishing episode.\")\n",
    "        print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
    "\n",
    "test_policy(env, pi.policy, num_episodes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pygame\n",
    "from io import StringIO\n",
    "from contextlib import closing\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces, Env\n",
    "from gymnasium.envs.toy_text.utils import categorical_sample\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/\n"
     ]
    }
   ],
   "source": [
    "# Set up TensorBoard logging\n",
    "log_dir = \"./logs/\"\n",
    "new_logger = configure(log_dir, [\"stdout\", \"tensorboard\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP = [\n",
    "    \"+---------+\",\n",
    "    \"|R: | : :G|\",\n",
    "    \"| :C| : :M|\",\n",
    "    \"| : : : : |\",\n",
    "    \"| | : | : |\",\n",
    "    \"|Y| : |B: |\",\n",
    "    \"+---------+\",\n",
    "]\n",
    "WINDOW_SIZE = (550, 350)\n",
    "FRAME_RATE = 10  # Frames per second\n",
    "DELAY = 0.1  # Delay in seconds\n",
    "\n",
    "class MultiAgentTaxiEnv(Env):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"ansi\", \"rgb_array\"],\n",
    "        \"render_fps\": FRAME_RATE,\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode=None, num_agents=2, num_tasks=3):\n",
    "        super(MultiAgentTaxiEnv, self).__init__()\n",
    "        self.desc = np.asarray(MAP, dtype=\"c\")\n",
    "        self.num_agents = num_agents\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.locs = locs = [(0, 0), (0, 4), (4, 0), (4, 3), (1, 2), (1, 5)]\n",
    "        self.locs_colors = [(255, 0, 0), (0, 255, 0), (255, 255, 0), (0, 0, 255), (0, 255, 255), (255, 0, 255)]\n",
    "\n",
    "        self.action_space = spaces.MultiDiscrete([6] * self.num_agents)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=5, shape=(self.num_agents * 3 + self.num_tasks * 4,), dtype=np.int32\n",
    "        )\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # pygame utils\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.cell_size = (\n",
    "            WINDOW_SIZE[0] / self.desc.shape[1],\n",
    "            WINDOW_SIZE[1] / self.desc.shape[0],\n",
    "        )\n",
    "        self.taxi_imgs = None\n",
    "        self.passenger_img = None\n",
    "        self.destination_img = None\n",
    "        self.median_horiz = None\n",
    "        self.median_vert = None\n",
    "        self.background_img = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.agents = np.zeros((self.num_agents, 3), dtype=np.int32)\n",
    "        self.agents[:, :2] = np.random.randint(0, 5, (self.num_agents, 2))\n",
    "\n",
    "        self.tasks = np.zeros((self.num_tasks, 4), dtype=np.int32)\n",
    "        for i in range(self.num_tasks):\n",
    "            pickup_loc = self.locs[np.random.randint(0, len(self.locs))]\n",
    "            dropoff_loc = self.locs[np.random.randint(0, len(self.locs))]\n",
    "            while dropoff_loc == pickup_loc:\n",
    "                dropoff_loc = self.locs[np.random.randint(0, len(self.locs))]\n",
    "            self.tasks[i] = np.array([pickup_loc[0], pickup_loc[1], dropoff_loc[0], dropoff_loc[1]])\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.concatenate([self.agents.flatten(), self.tasks.flatten()])\n",
    "\n",
    "    def step(self, actions):\n",
    "        assert self.action_space.contains(actions), f\"Invalid action: {actions}\"\n",
    "\n",
    "        rewards = np.zeros(self.num_agents)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        for i in range(self.num_agents):\n",
    "            if actions[i] == 0:  # Move up\n",
    "                self.agents[i, 1] = max(self.agents[i, 1] - 1, 0)\n",
    "            elif actions[i] == 1:  # Move down\n",
    "                self.agents[i, 1] = min(self.agents[i, 1] + 1, 4)\n",
    "            elif actions[i] == 2:  # Move left\n",
    "                self.agents[i, 0] = max(self.agents[i, 0] - 1, 0)\n",
    "            elif actions[i] == 3:  # Move right\n",
    "                self.agents[i, 0] = min(self.agents[i, 0] + 1, 4)\n",
    "            elif actions[i] == 4:  # Pick up task\n",
    "                for task_id, task in enumerate(self.tasks):\n",
    "                    if np.array_equal(self.agents[i, :2], task[:2]) and self.agents[i, 2] == 0:\n",
    "                        self.agents[i, 2] = task_id + 1  # Carry task_id + 1\n",
    "                        rewards[i] += 10  # Reward for successful pickup\n",
    "                        break  # Stop checking other tasks once one is picked up\n",
    "            elif actions[i] == 5:  # Drop off task\n",
    "                task_id = self.agents[i, 2] - 1\n",
    "                if task_id >= 0 and np.array_equal(self.agents[i, :2], self.tasks[task_id, 2:]):\n",
    "                    rewards[i] += 20  # Reward for successful drop-off\n",
    "                    self.tasks[task_id] = np.array([-1, -1, -1, -1])  # Invalidate the task\n",
    "                    self.agents[i, 2] = 0  # No longer carrying task\n",
    "                else:\n",
    "                    rewards[i] -= 10  # Penalty for illegal drop-off\n",
    "\n",
    "            # Penalize for each step to encourage efficiency\n",
    "            rewards[i] -= 1\n",
    "\n",
    "        # Check if all tasks are completed\n",
    "        if all(self.tasks[:, 0] == -1):\n",
    "            terminated = True\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        return self._get_obs(), np.sum(rewards), terminated, truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        try:\n",
    "            import pygame  # dependency to pygame only if rendering with human\n",
    "        except ImportError as e:\n",
    "            raise DependencyNotInstalled(\n",
    "                'pygame is not installed, run `pip install \"gymnasium[toy-text]\"`'\n",
    "            ) from e\n",
    "\n",
    "        if self.window is None:\n",
    "            pygame.init()\n",
    "            pygame.display.set_caption(\"Multi-Agent Taxi\")\n",
    "            if mode == \"human\":\n",
    "                self.window = pygame.display.set_mode(WINDOW_SIZE)\n",
    "            elif mode == \"rgb_array\":\n",
    "                self.window = pygame.Surface(WINDOW_SIZE)\n",
    "\n",
    "        assert (\n",
    "            self.window is not None\n",
    "        ), \"Something went wrong with pygame. This should never happen.\"\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "        if self.taxi_imgs is None:\n",
    "            file_names = [\n",
    "                \"img/cab_front.png\",\n",
    "                \"img/cab_rear.png\",\n",
    "                \"img/cab_right.png\",\n",
    "                \"img/cab_left.png\",\n",
    "            ]\n",
    "            self.taxi_imgs = [\n",
    "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
    "                for file_name in file_names\n",
    "            ]\n",
    "        if self.passenger_img is None:\n",
    "            file_name = \"img/passenger.png\"\n",
    "            self.passenger_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "        if self.destination_img is None:\n",
    "            file_name = \"img/hotel.png\"\n",
    "            self.destination_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "            self.destination_img.set_alpha(170)\n",
    "        if self.median_horiz is None:\n",
    "            file_names = [\n",
    "                \"img/gridworld_median_left.png\",\n",
    "                \"img/gridworld_median_horiz.png\",\n",
    "                \"img/gridworld_median_right.png\",\n",
    "            ]\n",
    "            self.median_horiz = [\n",
    "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
    "                for file_name in file_names\n",
    "            ]\n",
    "        if self.median_vert is None:\n",
    "            file_names = [\n",
    "                \"img/gridworld_median_top.png\",\n",
    "                \"img/gridworld_median_vert.png\",\n",
    "                \"img/gridworld_median_bottom.png\",\n",
    "            ]\n",
    "            self.median_vert = [\n",
    "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
    "                for file_name in file_names\n",
    "            ]\n",
    "        if self.background_img is None:\n",
    "            file_name = \"img/taxi_background.png\"\n",
    "            self.background_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "\n",
    "        desc = self.desc\n",
    "\n",
    "        for y in range(0, desc.shape[0]):\n",
    "            for x in range(0, desc.shape[1]):\n",
    "                cell = (x * self.cell_size[0], y * self.cell_size[1])\n",
    "                self.window.blit(self.background_img, cell)\n",
    "                if desc[y][x] == b\"|\" and (y == 0 or desc[y - 1][x] != b\"|\"):\n",
    "                    self.window.blit(self.median_vert[0], cell)\n",
    "                elif desc[y][x] == b\"|\" and (\n",
    "                    y == desc.shape[0] - 1 or desc[y + 1][x] != b\"|\"\n",
    "                ):\n",
    "                    self.window.blit(self.median_vert[2], cell)\n",
    "                elif desc[y][x] == b\"|\":\n",
    "                    self.window.blit(self.median_vert[1], cell)\n",
    "                elif desc[y][x] == b\"-\" and (x == 0 or desc[y][x - 1] != b\"-\"):\n",
    "                    self.window.blit(self.median_horiz[0], cell)\n",
    "                elif desc[y][x] == b\"-\" and (\n",
    "                    x == desc.shape[1] - 1 or desc[y][x + 1] != b\"-\"\n",
    "                ):\n",
    "                    self.window.blit(self.median_horiz[2], cell)\n",
    "                elif desc[y][x] == b\"-\":\n",
    "                    self.window.blit(self.median_horiz[1], cell)\n",
    "\n",
    "        for cell, color in zip(self.locs, self.locs_colors):\n",
    "            color_cell = pygame.Surface(self.cell_size)\n",
    "            color_cell.set_alpha(128)\n",
    "            color_cell.fill(color)\n",
    "            loc = self.get_surf_loc(cell)\n",
    "            self.window.blit(color_cell, (loc[0], loc[1] + 10))\n",
    "\n",
    "        for agent in self.agents:\n",
    "            self.window.blit(self.taxi_imgs[0], self.get_surf_loc((agent[0], agent[1])))\n",
    "\n",
    "        for task in self.tasks:\n",
    "            if task[0] != -1:\n",
    "                self.window.blit(self.passenger_img, self.get_surf_loc((task[0], task[1])))\n",
    "                self.window.blit(self.destination_img, self.get_surf_loc((task[2], task[3])))\n",
    "\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def get_surf_loc(self, map_loc):\n",
    "        return (map_loc[1] * 2 + 1) * self.cell_size[0], (\n",
    "            map_loc[0] + 1\n",
    "        ) * self.cell_size[1]\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            import pygame\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the environment\n",
    "# env = MultiAgentTaxiEnv(render_mode=\"human\", num_agents=2, num_tasks=3)\n",
    "# obs = env.reset()\n",
    "# done = False\n",
    "\n",
    "# DELAY = 0.1\n",
    "\n",
    "# while not done:\n",
    "#     action = env.action_space.sample()\n",
    "#     obs, reward, terminated, truncated, info = env.step(action)\n",
    "#     done = terminated or truncated\n",
    "#     env.render()\n",
    "#     time.sleep(DELAY)\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the environment\n",
    "base_env = MultiAgentTaxiEnv(render_mode=\"human\", num_agents=2, num_tasks=3)\n",
    "\n",
    "# Check if the environment follows the Gymnasium API\n",
    "check_env(base_env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = DummyVecEnv([lambda: Monitor(MultiAgentTaxiEnv(render_mode=\"human\", num_agents=2, num_tasks=3))])\n",
    "\n",
    "# # create the model\n",
    "# model = PPO('MlpPolicy', env, verbose=1)\n",
    "# model.set_logger(new_logger)\n",
    "\n",
    "# # Setup evaluation callback\n",
    "# eval_env = DummyVecEnv([lambda: Monitor(MultiAgentTaxiEnv(render_mode=\"human\", num_agents=2, num_tasks=3))])\n",
    "# eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
    "#                              log_path='./logs/', eval_freq=500,\n",
    "#                              deterministic=True, render=False)\n",
    "\n",
    "# # train the model\n",
    "# model.learn(total_timesteps=10000, callback=eval_callback)\n",
    "\n",
    "# # save the model\n",
    "# model.save(\"ppo_multi_agent_taxi\")\n",
    "\n",
    "# # load the model\n",
    "# model = PPO.load(\"ppo_multi_agent_taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and check the base environment\n",
    "base_env = MultiAgentTaxiEnv(render_mode=\"human\", num_agents=2, num_tasks=3)\n",
    "check_env(base_env, warn=True)\n",
    "\n",
    "# Initialize the custom environment and wrap it with Monitor and DummyVecEnv\n",
    "env = DummyVecEnv([lambda: Monitor(MultiAgentTaxiEnv(render_mode=\"human\", num_agents=2, num_tasks=3))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/\n",
      "Using cuda device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 175  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 11   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 170       |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 23        |\n",
      "|    total_timesteps      | 4096      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0284133 |\n",
      "|    clip_fraction        | 0.398     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.56     |\n",
      "|    explained_variance   | -0.00265  |\n",
      "|    learning_rate        | 0.003     |\n",
      "|    loss                 | 201       |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | -0.0388   |\n",
      "|    value_loss           | 2.61e+03  |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 36         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.27266628 |\n",
      "|    clip_fraction        | 0.433      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.47      |\n",
      "|    explained_variance   | -3.58e-07  |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | 88.6       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0071    |\n",
      "|    value_loss           | 851        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050143912 |\n",
      "|    clip_fraction        | 0.472       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.15       |\n",
      "|    explained_variance   | 2.98e-06    |\n",
      "|    learning_rate        | 0.003       |\n",
      "|    loss                 | 9.51        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.0196      |\n",
      "|    value_loss           | 26.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021555088 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.13       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.003       |\n",
      "|    loss                 | 11.4        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.0104      |\n",
      "|    value_loss           | 13          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034172624 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.09       |\n",
      "|    explained_variance   | -3.66e-05   |\n",
      "|    learning_rate        | 0.003       |\n",
      "|    loss                 | 2.4         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 0.00557     |\n",
      "|    value_loss           | 4.64        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035064556 |\n",
      "|    clip_fraction        | 0.424       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.07       |\n",
      "|    explained_variance   | 0.018       |\n",
      "|    learning_rate        | 0.003       |\n",
      "|    loss                 | 0.255       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.007       |\n",
      "|    value_loss           | 0.865       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 102         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038340315 |\n",
      "|    clip_fraction        | 0.47        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.04       |\n",
      "|    explained_variance   | 0.0417      |\n",
      "|    learning_rate        | 0.003       |\n",
      "|    loss                 | -0.235      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | 0.0062      |\n",
      "|    value_loss           | 0.753       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024477374 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.08       |\n",
      "|    explained_variance   | -0.00916    |\n",
      "|    learning_rate        | 0.003       |\n",
      "|    loss                 | 2.62        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | 0.00543     |\n",
      "|    value_loss           | 6.05        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 128         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042468525 |\n",
      "|    clip_fraction        | 0.432       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.09       |\n",
      "|    explained_variance   | -0.0158     |\n",
      "|    learning_rate        | 0.003       |\n",
      "|    loss                 | 1.18        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.00522     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 142         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042388633 |\n",
      "|    clip_fraction        | 0.424       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.08       |\n",
      "|    explained_variance   | -0.0148     |\n",
      "|    learning_rate        | 0.003       |\n",
      "|    loss                 | 2.83        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | 0.00795     |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 155         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076750435 |\n",
      "|    clip_fraction        | 0.494       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.16       |\n",
      "|    explained_variance   | -2.84       |\n",
      "|    learning_rate        | 0.003       |\n",
      "|    loss                 | -0.344      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 0.02        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 169         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.095890656 |\n",
      "|    clip_fraction        | 0.643       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.34       |\n",
      "|    explained_variance   | -0.00161    |\n",
      "|    learning_rate        | 0.003       |\n",
      "|    loss                 | 440         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | 0.0238      |\n",
      "|    value_loss           | 3.16e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mset_logger(new_logger)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_multi_agent_taxi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:224\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    221\u001b[0m             terminal_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mpredict_values(terminal_obs)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    222\u001b[0m         rewards[idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m terminal_value\n\u001b[0;32m--> 224\u001b[0m \u001b[43mrollout_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_episode_starts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m new_obs  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m dones\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:475\u001b[0m, in \u001b[0;36mRolloutBuffer.add\u001b[0;34m(self, obs, action, reward, episode_start, value, log_prob)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_starts[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(episode_start)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m--> 475\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_probs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m \u001b[43mlog_prob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set up TensorBoard logging\n",
    "log_dir = \"./logs/\"\n",
    "new_logger = configure(log_dir, [\"stdout\", \"tensorboard\"])\n",
    "\n",
    "# PPO model parameters\n",
    "total_timesteps = 1338900\n",
    "\n",
    "# Create the PPO model and set the logger\n",
    "model = PPO('MlpPolicy', env, verbose=1,\n",
    "            batch_size=64,\n",
    "            ent_coef=0.1,\n",
    "            gamma=0.985,\n",
    "            learning_rate=0.003,\n",
    "            n_steps=2048,\n",
    "            n_epochs=10,\n",
    "            policy_kwargs=dict(net_arch=[dict(pi=[64, 64], vf=[64, 64])]))\n",
    "\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_multi_agent_taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     11\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDELAY\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Control the speed of the simulation\u001b[39;00m\n\u001b[1;32m     14\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = PPO.load(\"ppo_multi_agent_taxi\")\n",
    "\n",
    "# test the model\n",
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    time.sleep(DELAY)  # Control the speed of the simulation\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
